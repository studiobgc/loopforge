"""
PitchEngine - Professional Pitch Detection & Correction

Features:
- Frame-by-frame pitch detection using pYIN algorithm
- Auto-tune style pitch correction with scale snapping
- Micro-pitch manipulation for width/chorus effects
- Formant-aware pitch shifting using phase vocoder
"""

import numpy as np
from dataclasses import dataclass
from typing import Optional, Callable
from pathlib import Path
import librosa


@dataclass  
class PitchContour:
    """Pitch detection result with frame-by-frame data."""
    times: np.ndarray           # Time in seconds for each frame
    frequencies: np.ndarray     # Detected pitch (Hz) per frame, 0 = unvoiced
    confidence: np.ndarray      # Confidence (0-1) per frame
    voiced_mask: np.ndarray     # Boolean mask for voiced frames
    
    @property
    def mean_pitch(self) -> float:
        """Mean pitch of voiced frames."""
        voiced = self.frequencies[self.voiced_mask]
        return float(np.mean(voiced)) if len(voiced) > 0 else 0.0
    
    @property  
    def pitch_range(self) -> tuple[float, float]:
        """Min/max pitch of voiced frames."""
        voiced = self.frequencies[self.voiced_mask]
        if len(voiced) == 0:
            return (0.0, 0.0)
        return (float(np.min(voiced)), float(np.max(voiced)))


class PitchEngine:
    """
    Advanced pitch detection and correction engine.
    
    Uses librosa's pYIN algorithm for robust pitch tracking,
    combined with phase vocoder pitch shifting for correction.
    
    Features:
    - High-accuracy pitch detection
    - Scale-aware auto-tune correction
    - Micro-pitch layering for width
    - Formant preservation options
    """
    
    def __init__(
        self,
        frame_length: int = 2048,
        hop_length: int = 512,
        fmin: float = 65.0,      # ~C2
        fmax: float = 2093.0     # ~C7
    ):
        """
        Initialize pitch engine.
        
        Args:
            frame_length: Analysis window size
            hop_length: Hop between frames
            fmin: Minimum detectable frequency
            fmax: Maximum detectable frequency
        """
        self.frame_length = frame_length
        self.hop_length = hop_length
        self.fmin = fmin
        self.fmax = fmax
    
    def detect_pitch(
        self,
        audio: np.ndarray,
        sr: int,
        confidence_threshold: float = 0.5
    ) -> PitchContour:
        """
        Detect pitch contour using CREPE (CNN-based pitch tracking).
        """
        import crepe
        
        # Ensure audio is mono and correct shape
        if audio.ndim > 1:
            audio = librosa.to_mono(audio)
            
        # CREPE expects float32
        if audio.dtype != np.float32:
            audio = audio.astype(np.float32)
            
        try:
            # Predict using CREPE
            # step_size=10ms (default) -> hop_length equivalent
            # viterbi=True for smoother contours
            time, frequency, confidence, activation = crepe.predict(
                audio, 
                sr, 
                viterbi=True, 
                step_size=10,
                verbose=0
            )
            
            # Create voiced mask based on confidence
            voiced_mask = (confidence >= confidence_threshold) & (frequency > 0)
            
            return PitchContour(
                times=time,
                frequencies=frequency,
                confidence=confidence,
                voiced_mask=voiced_mask
            )
            
        except Exception as e:
            print(f"[PITCH] CREPE detection failed: {e}")
            # Fallback to librosa pYIN
            return self._detect_pitch_librosa(audio, sr, confidence_threshold)

    def _detect_pitch_librosa(self, audio, sr, confidence_threshold):
        """Fallback CPU implementation."""
        if audio.ndim > 1:
            audio = librosa.to_mono(audio)
        
        f0, voiced_flag, voiced_probs = librosa.pyin(
            audio,
            sr=sr,
            fmin=self.fmin,
            fmax=self.fmax,
            frame_length=self.frame_length,
            hop_length=self.hop_length
        )
        times = librosa.times_like(f0, sr=sr, hop_length=self.hop_length)
        frequencies = np.nan_to_num(f0, nan=0.0)
        voiced_mask = (voiced_probs >= confidence_threshold) & (frequencies > 0)
        
        return PitchContour(times, frequencies, voiced_probs, voiced_mask)
    
    def correct_pitch(
        self,
        audio: np.ndarray,
        sr: int,
        target_scale: list[int],
        correction_strength: float = 1.0,
        preserve_vibrato: bool = True,
        vibrato_threshold_cents: float = 50.0
    ) -> np.ndarray:
        """
        Auto-tune style pitch correction.
        
        Args:
            audio: Input audio (mono)
            sr: Sample rate
            target_scale: List of pitch classes (0-11) to snap to
            correction_strength: 0.0 = no correction, 1.0 = full snap
            preserve_vibrato: Try to preserve natural vibrato
            vibrato_threshold_cents: Pitch deviation considered vibrato
            
        Returns:
            Pitch-corrected audio
        """
        # Detect pitch
        contour = self.detect_pitch(audio, sr)
        
        # Calculate required shifts
        shift_cents = np.zeros_like(contour.frequencies)
        
        for i, (freq, voiced) in enumerate(zip(contour.frequencies, contour.voiced_mask)):
            if not voiced or freq <= 0:
                continue
            
            # Find target pitch in scale
            _, cents_to_target = self._snap_to_scale(freq, target_scale)
            
            # Apply correction strength
            shift_cents[i] = cents_to_target * correction_strength
            
            # Vibrato preservation: reduce correction for small deviations
            if preserve_vibrato and abs(cents_to_target) < vibrato_threshold_cents:
                # Smooth transition near target pitch
                shift_cents[i] *= (abs(cents_to_target) / vibrato_threshold_cents)
        
        # Apply pitch shift using phase vocoder
        corrected = self._apply_pitch_shift_contour(audio, sr, shift_cents, contour.times)
        
        return corrected
    
    def shift_pitch(
        self,
        audio: np.ndarray,
        sr: int,
        semitones: float,
        preserve_formants: bool = True
    ) -> np.ndarray:
        """
        Shift pitch by a fixed amount using PyRubberband.
        
        Args:
            audio: Input audio
            sr: Sample rate
            semitones: Pitch shift amount (+/- semitones)
            preserve_formants: Try to preserve vocal formants
            
        Returns:
            Pitch-shifted audio
        """
        import pyrubberband as pyrb
        
        if preserve_formants:
            # PyRubberband handles formant preservation better than simple resampling
            # But strictly speaking, standard pitch_shift changes everything.
            # To preserve formants, we'd ideally use pyworld, but pyrb is a good upgrade from phase vocoder.
            # For true formant preservation, we use the ArtifactEngine's pyworld implementation.
            # Here we just use high-quality time-domain pitch shifting.
            shifted = pyrb.pitch_shift(audio, sr, n_steps=semitones)
        else:
            # Direct pitch shift
            shifted = pyrb.pitch_shift(audio, sr, n_steps=semitones)
        
        return shifted
    
    def create_pitch_layers(
        self,
        audio: np.ndarray,
        sr: int,
        detune_cents: float = 10.0,
        num_layers: int = 2,
        include_original: bool = True
    ) -> np.ndarray:
        """
        Create stereo-width effect using micro-pitch shifted layers.
        
        Args:
            audio: Input audio
            sr: Sample rate
            detune_cents: Amount to detune each layer (Â±)
            num_layers: Number of detuned layers per side
            include_original: Include the original in the mix
            
        Returns:
            Mixed audio with pitch layers
        """
        layers = []
        
        if include_original:
            layers.append(audio)
        
        # Create detuned copies
        for i in range(1, num_layers + 1):
            # Positive detune
            cents_up = detune_cents * i
            semitones_up = cents_up / 100.0
            layer_up = librosa.effects.pitch_shift(audio, sr=sr, n_steps=semitones_up)
            layers.append(layer_up * 0.7)  # Slightly quieter
            
            # Negative detune
            cents_down = -detune_cents * i
            semitones_down = cents_down / 100.0
            layer_down = librosa.effects.pitch_shift(audio, sr=sr, n_steps=semitones_down)
            layers.append(layer_down * 0.7)
        
        # Mix all layers
        mixed = np.sum(layers, axis=0)
        
        # Normalize to prevent clipping
        max_val = np.max(np.abs(mixed))
        if max_val > 0:
            mixed = mixed / max_val * 0.95
        
        return mixed
    
    def _snap_to_scale(
        self, 
        freq: float, 
        scale_pitches: list[int],
        reference_a4: float = 440.0
    ) -> tuple[float, float]:
        """
        Snap frequency to nearest pitch in scale.
        
        Returns:
            (target_freq, shift_cents)
        """
        if freq <= 0:
            return freq, 0.0
        
        # Convert to MIDI note
        midi = 12 * np.log2(freq / reference_a4) + 69
        
        # Get pitch class
        pc = int(round(midi)) % 12
        octave = int(round(midi)) // 12
        
        # Find nearest pitch class in scale
        min_dist = 12
        nearest_pc = pc
        
        for spc in scale_pitches:
            dist = min(abs(spc - pc), 12 - abs(spc - pc))
            if dist < min_dist:
                min_dist = dist
                nearest_pc = spc
        
        # Handle octave boundaries
        target_midi = octave * 12 + nearest_pc
        if nearest_pc < pc and pc - nearest_pc > 6:
            target_midi += 12
        elif nearest_pc > pc and nearest_pc - pc > 6:
            target_midi -= 12
        
        # Convert back to Hz
        target_freq = reference_a4 * (2 ** ((target_midi - 69) / 12))
        
        # Shift in cents
        shift_cents = 1200 * np.log2(target_freq / freq)
        
        return target_freq, shift_cents
    
    def _apply_pitch_shift_contour(
        self,
        audio: np.ndarray,
        sr: int,
        shift_cents: np.ndarray,
        times: np.ndarray
    ) -> np.ndarray:
        """
        Apply variable pitch shift based on contour.
        Uses overlap-add with per-frame pitch shifting.
        """
        # For simplicity, use average shift for now
        # TODO: Implement frame-by-frame PSOLA
        avg_shift = np.mean(shift_cents[shift_cents != 0])
        if np.isnan(avg_shift):
            avg_shift = 0
        
        semitones = avg_shift / 100.0
        
        if abs(semitones) < 0.01:
            return audio
        
        return librosa.effects.pitch_shift(audio, sr=sr, n_steps=semitones)
    
    def detect_from_file(self, filepath: Path, sr: int = None) -> PitchContour:
        """Load audio and detect pitch."""
        audio, file_sr = librosa.load(str(filepath), sr=sr, mono=True)
        return self.detect_pitch(audio, file_sr)
